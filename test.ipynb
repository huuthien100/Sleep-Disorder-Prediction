{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Đọc dữ liệu từ tệp tin CSV\n",
    "# file_path = 'Sleep_health_and_lifestyle_dataset.csv'\n",
    "# df = pd.read_csv(file_path)\n",
    "\n",
    "# # Chia tập dữ liệu thành features và class\n",
    "# class_column_name = 'Sleep Disorder'  # Thay bằng tên chính xác của cột lớp\n",
    "# features = df.drop(class_column_name, axis=1)\n",
    "# label = df[class_column_name]\n",
    "\n",
    "# # Hiển thị thông tin về features\n",
    "# print(\"\\nThông tin về features:\")\n",
    "# print(features.info())\n",
    "\n",
    "# # Hiển thị các giá trị duy nhất của class\n",
    "# print(\"\\nCác giá trị duy nhất của class:\")\n",
    "# print(label.unique())\n",
    "\n",
    "# # Hiển thị một số hàng đầu tiên của features và class\n",
    "# print(\"\\nMột số hàng đầu tiên của features:\")\n",
    "# print(features.head())\n",
    "\n",
    "# print(\"\\nMột số hàng đầu tiên của class:\")\n",
    "# print(label.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# import statistics\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "# import pandas as pd\n",
    "\n",
    "# # Record the start time for runtime measurement\n",
    "# start_time = time.time()\n",
    "\n",
    "# # Load the Sleep_health_and_lifestyle_dataset.csv\n",
    "# file_path = 'Sleep_health_and_lifestyle_dataset.csv'\n",
    "# df = pd.read_csv(file_path)\n",
    "\n",
    "# # Separate features and labels\n",
    "# X = df.drop('Sleep Disorder', axis=1)\n",
    "# y = df['Sleep Disorder']\n",
    "\n",
    "# # Convert symbolic values to numeric using LabelEncoder\n",
    "# label_encoder = LabelEncoder()\n",
    "# for column in X.columns:\n",
    "#     X[column] = label_encoder.fit_transform(X[column])\n",
    "\n",
    "# # Convert labels to numeric format\n",
    "# y = label_encoder.fit_transform(y)\n",
    "\n",
    "# # Use StratifiedKFold with k=5 and k=10\n",
    "# for k in [5, 10]:\n",
    "#     # Initialize arrays and variables\n",
    "#     Mean_Acc_svm, Mean_Prec_svm, Mean_Rec_svm, Mean_F1_svm = [], [], [], []\n",
    "#     Mean_Acc_rf, Mean_Prec_rf, Mean_Rec_rf, Mean_F1_rf = [], [], [], []\n",
    "#     Mean_Acc_dt, Mean_Prec_dt, Mean_Rec_dt, Mean_F1_dt = [], [], [], []\n",
    "#     Mean_Acc_nb, Mean_Prec_nb, Mean_Rec_nb, Mean_F1_nb = [], [], [], []\n",
    "#     Mean_Acc_knn, Mean_Prec_knn, Mean_Rec_knn, Mean_F1_knn = [], [], [], []\n",
    "#     Mean_Acc_gb, Mean_Prec_gb, Mean_Rec_gb, Mean_F1_gb = [], [], [], []\n",
    "\n",
    "#     # Initialize StratifiedKFold\n",
    "#     kf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "#     # Iterate through folds\n",
    "#     for train_index, test_index in kf.split(X, y):\n",
    "#         X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "#         y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "#         # Initialize and train the SVM model with default parameters\n",
    "#         SVM_Model = SVC()\n",
    "#         SVM_Model.fit(X_train, y_train)\n",
    "#         y_pred_svm = SVM_Model.predict(X_test)\n",
    "\n",
    "#         # Evaluate the performance of the model\n",
    "#         Accuracy_svm = accuracy_score(y_test, y_pred_svm) * 100\n",
    "#         Precision_svm = precision_score(y_test, y_pred_svm, average='weighted', zero_division=0) * 100\n",
    "#         Recall_svm = recall_score(y_test, y_pred_svm, average='weighted', zero_division=0) * 100\n",
    "#         F1_svm = f1_score(y_test, y_pred_svm, average='weighted', zero_division=0) * 100\n",
    "\n",
    "#         # Append metrics to lists\n",
    "#         Mean_Acc_svm.append(Accuracy_svm)\n",
    "#         Mean_Prec_svm.append(Precision_svm)\n",
    "#         Mean_Rec_svm.append(Recall_svm)\n",
    "#         Mean_F1_svm.append(F1_svm)\n",
    "\n",
    "#         # Initialize and train the Naive Bayes model\n",
    "#         NB_Model = GaussianNB()\n",
    "#         NB_Model.fit(X_train, y_train)\n",
    "#         y_pred_nb = NB_Model.predict(X_test)\n",
    "\n",
    "#         # Evaluate the performance of the model\n",
    "#         Accuracy_nb = accuracy_score(y_test, y_pred_nb) * 100\n",
    "#         Precision_nb = precision_score(y_test, y_pred_nb, average='weighted', zero_division=0) * 100\n",
    "#         Recall_nb = recall_score(y_test, y_pred_nb, average='weighted', zero_division=0) * 100\n",
    "#         F1_nb = f1_score(y_test, y_pred_nb, average='weighted', zero_division=0) * 100\n",
    "\n",
    "#         # Append metrics to lists\n",
    "#         Mean_Acc_nb.append(Accuracy_nb)\n",
    "#         Mean_Prec_nb.append(Precision_nb)\n",
    "#         Mean_Rec_nb.append(Recall_nb)\n",
    "#         Mean_F1_nb.append(F1_nb)\n",
    "\n",
    "#         # Initialize and train the Decision Tree model\n",
    "#         DT_Model = DecisionTreeClassifier()\n",
    "#         DT_Model.fit(X_train, y_train)\n",
    "\n",
    "#         # Evaluate the performance of the model\n",
    "#         Accuracy_dt = accuracy_score(y_test, DT_Model.predict(X_test)) * 100\n",
    "#         Precision_dt = precision_score(y_test, DT_Model.predict(X_test), average='weighted', zero_division=0) * 100\n",
    "#         Recall_dt = recall_score(y_test, DT_Model.predict(X_test), average='weighted', zero_division=0) * 100\n",
    "#         F1_dt = f1_score(y_test, DT_Model.predict(X_test), average='weighted', zero_division=0) * 100\n",
    "\n",
    "#         # Append metrics to lists\n",
    "#         Mean_Acc_dt.append(Accuracy_dt)\n",
    "#         Mean_Prec_dt.append(Precision_dt)\n",
    "#         Mean_Rec_dt.append(Recall_dt)\n",
    "#         Mean_F1_dt.append(F1_dt)\n",
    "\n",
    "#         # Initialize and train the Random Forest model\n",
    "#         RF_Model = RandomForestClassifier()\n",
    "#         RF_Model.fit(X_train, y_train)\n",
    "#         y_pred_rf = RF_Model.predict(X_test)\n",
    "\n",
    "#         # Evaluate the performance of the model\n",
    "#         Accuracy_rf = accuracy_score(y_test, y_pred_rf) * 100\n",
    "#         Precision_rf = precision_score(y_test, y_pred_rf, average='weighted', zero_division=0) * 100\n",
    "#         Recall_rf = recall_score(y_test, y_pred_rf, average='weighted', zero_division=0) * 100\n",
    "#         F1_rf = f1_score(y_test, y_pred_rf, average='weighted', zero_division=0) * 100\n",
    "\n",
    "#         # Append metrics to lists\n",
    "#         Mean_Acc_rf.append(Accuracy_rf)\n",
    "#         Mean_Prec_rf.append(Precision_rf)\n",
    "#         Mean_Rec_rf.append(Recall_rf)\n",
    "#         Mean_F1_rf.append(F1_rf)\n",
    "\n",
    "#         # Initialize and train the k-Nearest Neighbors model\n",
    "#         KNN_Model = KNeighborsClassifier()\n",
    "#         KNN_Model.fit(X_train, y_train)\n",
    "#         y_pred_knn = KNN_Model.predict(X_test)\n",
    "\n",
    "#         # Evaluate the performance of the model\n",
    "#         Accuracy_knn = accuracy_score(y_test, y_pred_knn) * 100\n",
    "#         Precision_knn = precision_score(y_test, y_pred_knn, average='weighted', zero_division=0) * 100\n",
    "#         Recall_knn = recall_score(y_test, y_pred_knn, average='weighted', zero_division=0) * 100\n",
    "#         F1_knn = f1_score(y_test, y_pred_knn, average='weighted', zero_division=0) * 100\n",
    "\n",
    "#         # Append metrics to lists\n",
    "#         Mean_Acc_knn.append(Accuracy_knn)\n",
    "#         Mean_Prec_knn.append(Precision_knn)\n",
    "#         Mean_Rec_knn.append(Recall_knn)\n",
    "#         Mean_F1_knn.append(F1_knn)\n",
    "\n",
    "#         # Initialize and train the Gradient Boosting model\n",
    "#         GB_Model = GradientBoostingClassifier()\n",
    "#         GB_Model.fit(X_train, y_train)\n",
    "#         y_pred_gb = GB_Model.predict(X_test)\n",
    "\n",
    "#         # Evaluate the performance of the model\n",
    "#         Accuracy_gb = accuracy_score(y_test, y_pred_gb) * 100\n",
    "#         Precision_gb = precision_score(y_test, y_pred_gb, average='weighted', zero_division=0) * 100\n",
    "#         Recall_gb = recall_score(y_test, y_pred_gb, average='weighted', zero_division=0) * 100\n",
    "#         F1_gb = f1_score(y_test, y_pred_gb, average='weighted', zero_division=0) * 100\n",
    "\n",
    "#         # Append metrics to lists\n",
    "#         Mean_Acc_gb.append(Accuracy_gb)\n",
    "#         Mean_Prec_gb.append(Precision_gb)\n",
    "#         Mean_Rec_gb.append(Recall_gb)\n",
    "#         Mean_F1_gb.append(F1_gb)\n",
    "\n",
    "#     # Calculate average metrics over all iterations for SVM\n",
    "#     Acc_SVM = statistics.mean(Mean_Acc_svm)\n",
    "#     Prec_SVM = statistics.mean(Mean_Prec_svm)\n",
    "#     Rec_SVM = statistics.mean(Mean_Rec_svm)\n",
    "#     F1_SVM = statistics.mean(Mean_F1_svm)\n",
    "\n",
    "#     # Calculate average metrics over all iterations for Naive Bayes\n",
    "#     Acc_NB = statistics.mean(Mean_Acc_nb)\n",
    "#     Prec_NB = statistics.mean(Mean_Prec_nb)\n",
    "#     Rec_NB = statistics.mean(Mean_Rec_nb)\n",
    "#     F1_NB = statistics.mean(Mean_F1_nb)\n",
    "\n",
    "#     # Calculate average metrics over all iterations for Decision Tree\n",
    "#     Acc_DT = statistics.mean(Mean_Acc_dt)\n",
    "#     Prec_DT = statistics.mean(Mean_Prec_dt)\n",
    "#     Rec_DT = statistics.mean(Mean_Rec_dt)\n",
    "#     F1_DT = statistics.mean(Mean_F1_dt)\n",
    "\n",
    "#     # Calculate average metrics over all iterations for Random Forest\n",
    "#     Acc_RF = statistics.mean(Mean_Acc_rf)\n",
    "#     Prec_RF = statistics.mean(Mean_Prec_rf)\n",
    "#     Rec_RF = statistics.mean(Mean_Rec_rf)\n",
    "#     F1_RF = statistics.mean(Mean_F1_rf)\n",
    "\n",
    "#     # Calculate average metrics over all iterations for k-Nearest Neighbors\n",
    "#     Acc_KNN = statistics.mean(Mean_Acc_knn)\n",
    "#     Prec_KNN = statistics.mean(Mean_Prec_knn)\n",
    "#     Rec_KNN = statistics.mean(Mean_Rec_knn)\n",
    "#     F1_KNN = statistics.mean(Mean_F1_knn)\n",
    "\n",
    "#     # Calculate average metrics over all iterations for Gradient Boosting\n",
    "#     Acc_GB = statistics.mean(Mean_Acc_gb)\n",
    "#     Prec_GB = statistics.mean(Mean_Prec_gb)\n",
    "#     Rec_GB = statistics.mean(Mean_Rec_gb)\n",
    "#     F1_GB = statistics.mean(Mean_F1_gb)\n",
    "\n",
    "#     # Draw a table using plt.table for K-Fold Results\n",
    "#     results_table_kfold = [\n",
    "#         ['Model', 'K', 'Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "#         ['SVM', k, f'{Acc_SVM:.4f}', f'{Prec_SVM:.4f}', f'{Rec_SVM:.4f}', f'{F1_SVM:.4f}'],\n",
    "#         ['Naive Bayes', k, f'{Acc_NB:.4f}', f'{Prec_NB:.4f}', f'{Rec_NB:.4f}', f'{F1_NB:.4f}'],\n",
    "#         ['Decision Tree', k, f'{Acc_DT:.4f}', f'{Prec_DT:.4f}', f'{Rec_DT:.4f}', f'{F1_DT:.4f}'],\n",
    "#         ['Random Forest', k, f'{Acc_RF:.4f}', f'{Prec_RF:.4f}', f'{Rec_RF:.4f}', f'{F1_RF:.4f}'],\n",
    "#         ['k-Nearest Neighbors', k, f'{Acc_KNN:.4f}', f'{Prec_KNN:.4f}', f'{Rec_KNN:.4f}', f'{F1_KNN:.4f}'],\n",
    "#         ['Gradient Boosting', k, f'{Acc_GB:.4f}', f'{Prec_GB:.4f}', f'{Rec_GB:.4f}', f'{F1_GB:.4f}']\n",
    "#     ]\n",
    "\n",
    "#     table_kfold = plt.table(cellText=results_table_kfold, loc='center', cellLoc='center', colLabels=None, cellColours=None)\n",
    "#     table_kfold.auto_set_font_size(False)\n",
    "#     table_kfold.set_fontsize(12)\n",
    "#     table_kfold.scale(2, 2)\n",
    "#     plt.title('K-Fold Results')\n",
    "\n",
    "#     plt.axis('off')\n",
    "#     plt.show()\n",
    "\n",
    "# end_time = time.time()\n",
    "# print(f'\\nRuntime: {end_time - start_time:.4f} seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import statistics\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Record the start time for overall runtime measurement\n",
    "start_time = time.time()\n",
    "\n",
    "# Load the Sleep_health_and_lifestyle_dataset.csv\n",
    "file_path = 'Sleep_health_and_lifestyle_dataset.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Separate features and labels\n",
    "X = df.drop('Sleep Disorder', axis=1)\n",
    "y = df['Sleep Disorder']\n",
    "\n",
    "# Convert symbolic values to numeric using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "for column in X.columns:\n",
    "    X[column] = label_encoder.fit_transform(X[column])\n",
    "\n",
    "# Convert labels to numeric format\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Lists to store metrics\n",
    "results_kfold, results_holdout = [], []\n",
    "\n",
    "for param_set in [\n",
    "    {'max_depth': 3, 'min_samples_split': 2},\n",
    "    {'max_depth': 3, 'min_samples_split': 3},\n",
    "    {'max_depth': 4, 'min_samples_split': 4},\n",
    "    {'max_depth': 5, 'min_samples_split': 3},\n",
    "    {'max_depth': None, 'min_samples_split': 2},\n",
    "]:\n",
    "    # Initialize arrays to store metrics for the current parameter set\n",
    "    Mean_Acc_kfold, Mean_Prec_kfold, Mean_Rec_kfold, Mean_F1_kfold = [], [], [], []\n",
    "    Mean_Acc_holdout, Mean_Prec_holdout, Mean_Rec_holdout, Mean_F1_holdout = [], [], [], []\n",
    "\n",
    "    # K-fold cross-validation\n",
    "    kf = StratifiedKFold(n_splits=50, shuffle=True, random_state=42)\n",
    "\n",
    "    for train_index, test_index in kf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Initialize and train the Decision Tree model\n",
    "        tree_model = DecisionTreeClassifier(max_depth=param_set['max_depth'], min_samples_split=param_set['min_samples_split'])\n",
    "        tree_model.fit(X_train, y_train)\n",
    "        y_pred = tree_model.predict(X_test)\n",
    "\n",
    "        # Evaluate the performance of the model for k-fold\n",
    "        Accuracy = round(accuracy_score(y_test, y_pred) * 100, 4)\n",
    "        Precision = round(precision_score(y_test, y_pred, average='weighted', zero_division=0) * 100, 4)\n",
    "        Recall = round(recall_score(y_test, y_pred, average='weighted', zero_division=0) * 100, 4)\n",
    "        F1 = round(f1_score(y_test, y_pred, average='weighted', zero_division=0) * 100, 4)\n",
    "\n",
    "        # Append metrics to lists\n",
    "        Mean_Acc_kfold.append(Accuracy)\n",
    "        Mean_Prec_kfold.append(Precision)\n",
    "        Mean_Rec_kfold.append(Recall)\n",
    "        Mean_F1_kfold.append(F1)\n",
    "\n",
    "    # Holdout evaluation\n",
    "    for _ in range(50):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/4, random_state=42)\n",
    "\n",
    "        # Initialize and train the Decision Tree model\n",
    "        tree_model = DecisionTreeClassifier(max_depth=param_set['max_depth'], min_samples_split=param_set['min_samples_split'])\n",
    "        tree_model.fit(X_train, y_train)\n",
    "        y_pred = tree_model.predict(X_test)\n",
    "\n",
    "        # Evaluate the performance of the model for holdout\n",
    "        Accuracy = round(accuracy_score(y_test, y_pred) * 100, 4)\n",
    "        Precision = round(precision_score(y_test, y_pred, average='weighted', zero_division=0) * 100, 4)\n",
    "        Recall = round(recall_score(y_test, y_pred, average='weighted', zero_division=0) * 100, 4)\n",
    "        F1 = round(f1_score(y_test, y_pred, average='weighted', zero_division=0) * 100, 4)\n",
    "\n",
    "        # Append metrics to lists\n",
    "        Mean_Acc_holdout.append(Accuracy)\n",
    "        Mean_Prec_holdout.append(Precision)\n",
    "        Mean_Rec_holdout.append(Recall)\n",
    "        Mean_F1_holdout.append(F1)\n",
    "\n",
    "    # Calculate mean metrics for k-fold\n",
    "    mean_accuracy_kfold = round(statistics.mean(Mean_Acc_kfold), 4)\n",
    "    mean_precision_kfold = round(statistics.mean(Mean_Prec_kfold), 4)\n",
    "    mean_recall_kfold = round(statistics.mean(Mean_Rec_kfold), 4)\n",
    "    mean_f1_kfold = round(statistics.mean(Mean_F1_kfold), 4)\n",
    "\n",
    "    # Calculate mean metrics for holdout\n",
    "    mean_accuracy_holdout = round(statistics.mean(Mean_Acc_holdout), 4)\n",
    "    mean_precision_holdout = round(statistics.mean(Mean_Prec_holdout), 4)\n",
    "    mean_recall_holdout = round(statistics.mean(Mean_Rec_holdout), 4)\n",
    "    mean_f1_holdout = round(statistics.mean(Mean_F1_holdout), 4)\n",
    "\n",
    "    # Store results for the current parameter set\n",
    "    results_kfold.append({\n",
    "        'Parameters': param_set,\n",
    "        'Mean Accuracy K-Fold': mean_accuracy_kfold,\n",
    "        'Mean Precision_K-Fold': mean_precision_kfold,\n",
    "        'Mean Recall_K-Fold': mean_recall_kfold,\n",
    "        'Mean F1 K-Fold': mean_f1_kfold,\n",
    "    })\n",
    "\n",
    "    results_holdout.append({\n",
    "        'Parameters': param_set,\n",
    "        'Mean Accuracy Holdout': mean_accuracy_holdout,\n",
    "        'Mean Precision Holdout': mean_precision_holdout,\n",
    "        'Mean Recall Holdout': mean_recall_holdout,\n",
    "        'Mean F1 Holdout': mean_f1_holdout,\n",
    "    })\n",
    "\n",
    "# Create Pandas DataFrames for better visualization\n",
    "results_kfold_df = pd.DataFrame(results_kfold)\n",
    "results_holdout_df = pd.DataFrame(results_holdout)\n",
    "\n",
    "# Draw the k-fold table\n",
    "table_kfold = plt.table(cellText=results_kfold_df.values, colLabels=results_kfold_df.columns, cellLoc='center', loc='center')\n",
    "table_kfold.auto_set_font_size(False)\n",
    "table_kfold.auto_set_column_width(col=list(range(len(results_holdout_df.columns))))\n",
    "table_kfold.set_fontsize(12)\n",
    "table_kfold.scale(2, 2)\n",
    "plt.title('K-Fold Results - Decision Tree')\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Draw the holdout table\n",
    "table_holdout = plt.table(cellText=results_holdout_df.values, colLabels=results_holdout_df.columns, cellLoc='center', loc='center')\n",
    "table_holdout.auto_set_font_size(False)\n",
    "table_holdout.auto_set_column_width(col=list(range(len(results_holdout_df.columns))))\n",
    "table_holdout.set_fontsize(12)\n",
    "table_holdout.scale(2, 2)\n",
    "plt.title('Holdout Results - Decision Tree')\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "end_time = time.time()\n",
    "print(f'\\nRuntime: {end_time - start_time:.4f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import statistics\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Record the start time for overall runtime measurement\n",
    "start_time_overall = time.time()\n",
    "\n",
    "# Load the Sleep_health_and_lifestyle_dataset.csv\n",
    "file_path = 'Sleep_health_and_lifestyle_dataset.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Separate features and labels\n",
    "X = df.drop('Sleep Disorder', axis=1)\n",
    "y = df['Sleep Disorder']\n",
    "\n",
    "# Convert symbolic values to numeric using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "for column in X.columns:\n",
    "    X[column] = label_encoder.fit_transform(X[column])\n",
    "\n",
    "# Convert labels to numeric format\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Lists to store metrics\n",
    "results_kfold, results_holdout = [], []\n",
    "\n",
    "for param_set in [\n",
    "    {'n_estimators': 100, 'max_depth': 3, 'min_samples_split': 2},\n",
    "    {'n_estimators': 100, 'max_depth': 3, 'min_samples_split': 3},\n",
    "    {'n_estimators': 100, 'max_depth': 4, 'min_samples_split': 4},\n",
    "    {'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 3},\n",
    "    {'n_estimators': 100, 'max_depth': None, 'min_samples_split': 2},\n",
    "]:\n",
    "    # Initialize arrays to store metrics for the current parameter set\n",
    "    Mean_Acc_kfold, Mean_Prec_kfold, Mean_Rec_kfold, Mean_F1_kfold = [], [], [], []\n",
    "    Mean_Acc_holdout, Mean_Prec_holdout, Mean_Rec_holdout, Mean_F1_holdout = [], [], [], []\n",
    "\n",
    "    # K-fold cross-validation\n",
    "    kf = StratifiedKFold(n_splits=50, shuffle=True, random_state=42)\n",
    "\n",
    "    for train_index, test_index in kf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Initialize and train the Random Forest model\n",
    "        forest_model = RandomForestClassifier(\n",
    "            n_estimators=param_set['n_estimators'],\n",
    "            max_depth=param_set['max_depth'],\n",
    "            min_samples_split=param_set['min_samples_split']\n",
    "        )\n",
    "        forest_model.fit(X_train, y_train)\n",
    "        y_pred = forest_model.predict(X_test)\n",
    "\n",
    "        # Evaluate the performance of the model for k-fold\n",
    "        Accuracy = round(accuracy_score(y_test, y_pred) * 100, 4)\n",
    "        Precision = round(precision_score(y_test, y_pred, average='weighted', zero_division=0) * 100, 4)\n",
    "        Recall = round(recall_score(y_test, y_pred, average='weighted', zero_division=0) * 100, 4)\n",
    "        F1 = round(f1_score(y_test, y_pred, average='weighted', zero_division=0) * 100, 4)\n",
    "\n",
    "        # Append metrics to lists\n",
    "        Mean_Acc_kfold.append(Accuracy)\n",
    "        Mean_Prec_kfold.append(Precision)\n",
    "        Mean_Rec_kfold.append(Recall)\n",
    "        Mean_F1_kfold.append(F1)\n",
    "\n",
    "    # Holdout evaluation\n",
    "    for _ in range(50):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/4, random_state=42)\n",
    "\n",
    "        # Initialize and train the Random Forest model\n",
    "        forest_model = RandomForestClassifier(\n",
    "            n_estimators=param_set['n_estimators'],\n",
    "            max_depth=param_set['max_depth'],\n",
    "            min_samples_split=param_set['min_samples_split']\n",
    "        )\n",
    "        forest_model.fit(X_train, y_train)\n",
    "        y_pred = forest_model.predict(X_test)\n",
    "\n",
    "        # Evaluate the performance of the model for holdout\n",
    "        Accuracy = round(accuracy_score(y_test, y_pred) * 100, 4)\n",
    "        Precision = round(precision_score(y_test, y_pred, average='weighted', zero_division=0) * 100, 4)\n",
    "        Recall = round(recall_score(y_test, y_pred, average='weighted', zero_division=0) * 100, 4)\n",
    "        F1 = round(f1_score(y_test, y_pred, average='weighted', zero_division=0) * 100, 4)\n",
    "\n",
    "        # Append metrics to lists\n",
    "        Mean_Acc_holdout.append(Accuracy)\n",
    "        Mean_Prec_holdout.append(Precision)\n",
    "        Mean_Rec_holdout.append(Recall)\n",
    "        Mean_F1_holdout.append(F1)\n",
    "\n",
    "    # Calculate mean metrics for k-fold\n",
    "    mean_accuracy_kfold = round(statistics.mean(Mean_Acc_kfold), 4)\n",
    "    mean_precision_kfold = round(statistics.mean(Mean_Prec_kfold), 4)\n",
    "    mean_recall_kfold = round(statistics.mean(Mean_Rec_kfold), 4)\n",
    "    mean_f1_kfold = round(statistics.mean(Mean_F1_kfold), 4)\n",
    "\n",
    "    # Calculate mean metrics for holdout\n",
    "    mean_accuracy_holdout = round(statistics.mean(Mean_Acc_holdout), 4)\n",
    "    mean_precision_holdout = round(statistics.mean(Mean_Prec_holdout), 4)\n",
    "    mean_recall_holdout = round(statistics.mean(Mean_Rec_holdout), 4)\n",
    "    mean_f1_holdout = round(statistics.mean(Mean_F1_holdout), 4)\n",
    "\n",
    "    # Store results for the current parameter set\n",
    "    results_kfold.append({\n",
    "        'Parameters': param_set,\n",
    "        'Mean Accuracy K-Fold': mean_accuracy_kfold,\n",
    "        'Mean Precision_K-Fold': mean_precision_kfold,\n",
    "        'Mean Recall_K-Fold': mean_recall_kfold,\n",
    "        'Mean F1 K-Fold': mean_f1_kfold,\n",
    "    })\n",
    "\n",
    "    results_holdout.append({\n",
    "        'Parameters': param_set,\n",
    "        'Mean Accuracy Holdout': mean_accuracy_holdout,\n",
    "        'Mean Precision Holdout': mean_precision_holdout,\n",
    "        'Mean Recall Holdout': mean_recall_holdout,\n",
    "        'Mean F1 Holdout': mean_f1_holdout,\n",
    "    })\n",
    "\n",
    "# Create Pandas DataFrames for better visualization\n",
    "results_kfold_df = pd.DataFrame(results_kfold)\n",
    "results_holdout_df = pd.DataFrame(results_holdout)\n",
    "\n",
    "# Draw the k-fold table\n",
    "table_kfold = plt.table(cellText=results_kfold_df.values, colLabels=results_kfold_df.columns, cellLoc='center', loc='center')\n",
    "table_kfold.auto_set_font_size(False)\n",
    "table_kfold.auto_set_column_width(col=list(range(len(results_holdout_df.columns))))\n",
    "table_kfold.set_fontsize(12)\n",
    "table_kfold.scale(2, 2)\n",
    "plt.title('K-Fold Results - Random Forest')\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Draw the holdout table\n",
    "table_holdout = plt.table(cellText=results_holdout_df.values, colLabels=results_holdout_df.columns, cellLoc='center', loc='center')\n",
    "table_holdout.auto_set_font_size(False)\n",
    "table_holdout.auto_set_column_width(col=list(range(len(results_holdout_df.columns))))\n",
    "table_holdout.set_fontsize(12)\n",
    "table_holdout.scale(2, 2)\n",
    "plt.title('Holdout Results - Random Forest')\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "end_time = time.time()\n",
    "print(f'\\nRuntime: {end_time - start_time:.4f} seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import statistics\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import joblib\n",
    "\n",
    "# Record the start time for overall runtime measurement\n",
    "start_time_overall = time.time()\n",
    "\n",
    "# Load the Sleep_health_and_lifestyle_dataset.csv\n",
    "file_path = 'Sleep_health_and_lifestyle_dataset.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Remove the first column ('person ID')\n",
    "df = df.iloc[:, 1:]\n",
    "\n",
    "# Tách cột \"Blood Pressure\" thành \"Systolic\" và \"Diastolic\"\n",
    "df[['Systolic', 'Diastolic']] = df['Blood Pressure'].str.split('/', expand=True)\n",
    "\n",
    "# Chuyển đổi kiểu dữ liệu sang numeric\n",
    "df['Systolic'] = pd.to_numeric(df['Systolic'], errors='coerce')\n",
    "df['Diastolic'] = pd.to_numeric(df['Diastolic'], errors='coerce')\n",
    "\n",
    "# Xóa cột \"Blood Pressure\" ban đầu\n",
    "df = df.drop('Blood Pressure', axis=1)\n",
    "\n",
    "# Separate features and labels\n",
    "X = df.drop('Sleep Disorder', axis=1)\n",
    "y = df['Sleep Disorder']\n",
    "\n",
    "# Convert symbolic values to numeric using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "for column in X.columns:\n",
    "    X[column] = label_encoder.fit_transform(X[column])\n",
    "\n",
    "# Convert labels to numeric format\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Lists to store metrics\n",
    "results_kfold, results_holdout = [], []\n",
    "\n",
    "for param_set in [\n",
    "    {'n_estimators': 100, 'max_depth': 3, 'min_samples_split': 2},\n",
    "    {'n_estimators': 100, 'max_depth': 3, 'min_samples_split': 3},\n",
    "    {'n_estimators': 100, 'max_depth': 4, 'min_samples_split': 4},\n",
    "    {'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 3},\n",
    "    {'n_estimators': 100, 'max_depth': None, 'min_samples_split': 2},\n",
    "]:\n",
    "    # Initialize arrays to store metrics for the current parameter set\n",
    "    Mean_Acc_kfold, Mean_Prec_kfold, Mean_Rec_kfold, Mean_F1_kfold = [], [], [], []\n",
    "    Mean_Acc_holdout, Mean_Prec_holdout, Mean_Rec_holdout, Mean_F1_holdout = [], [], [], []\n",
    "\n",
    "    # K-fold cross-validation\n",
    "    kf = StratifiedKFold(n_splits=50, shuffle=True, random_state=42)\n",
    "\n",
    "    for train_index, test_index in kf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Initialize and train the Random Forest model\n",
    "        forest_model = RandomForestClassifier(\n",
    "            n_estimators=param_set['n_estimators'],\n",
    "            max_depth=param_set['max_depth'],\n",
    "            min_samples_split=param_set['min_samples_split']\n",
    "        )\n",
    "        forest_model.fit(X_train, y_train)\n",
    "        y_pred = forest_model.predict(X_test)\n",
    "\n",
    "        # Evaluate the performance of the model for k-fold\n",
    "        Accuracy = round(accuracy_score(y_test, y_pred) * 100, 4)\n",
    "        Precision = round(precision_score(y_test, y_pred, average='weighted', zero_division=0) * 100, 4)\n",
    "        Recall = round(recall_score(y_test, y_pred, average='weighted', zero_division=0) * 100, 4)\n",
    "        F1 = round(f1_score(y_test, y_pred, average='weighted', zero_division=0) * 100, 4)\n",
    "\n",
    "        # Append metrics to lists\n",
    "        Mean_Acc_kfold.append(Accuracy)\n",
    "        Mean_Prec_kfold.append(Precision)\n",
    "        Mean_Rec_kfold.append(Recall)\n",
    "        Mean_F1_kfold.append(F1)\n",
    "\n",
    "    # Holdout evaluation\n",
    "    for _ in range(50):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/4, random_state=42)\n",
    "\n",
    "        # Initialize and train the Random Forest model\n",
    "        forest_model = RandomForestClassifier(\n",
    "            n_estimators=param_set['n_estimators'],\n",
    "            max_depth=param_set['max_depth'],\n",
    "            min_samples_split=param_set['min_samples_split']\n",
    "        )\n",
    "        forest_model.fit(X_train, y_train)\n",
    "        y_pred = forest_model.predict(X_test)\n",
    "\n",
    "        # Evaluate the performance of the model for holdout\n",
    "        Accuracy = round(accuracy_score(y_test, y_pred) * 100, 4)\n",
    "        Precision = round(precision_score(y_test, y_pred, average='weighted', zero_division=0) * 100, 4)\n",
    "        Recall = round(recall_score(y_test, y_pred, average='weighted', zero_division=0) * 100, 4)\n",
    "        F1 = round(f1_score(y_test, y_pred, average='weighted', zero_division=0) * 100, 4)\n",
    "\n",
    "        # Append metrics to lists\n",
    "        Mean_Acc_holdout.append(Accuracy)\n",
    "        Mean_Prec_holdout.append(Precision)\n",
    "        Mean_Rec_holdout.append(Recall)\n",
    "        Mean_F1_holdout.append(F1)\n",
    "\n",
    "    # Calculate mean metrics for k-fold\n",
    "    mean_accuracy_kfold = round(statistics.mean(Mean_Acc_kfold), 4)\n",
    "    mean_precision_kfold = round(statistics.mean(Mean_Prec_kfold), 4)\n",
    "    mean_recall_kfold = round(statistics.mean(Mean_Rec_kfold), 4)\n",
    "    mean_f1_kfold = round(statistics.mean(Mean_F1_kfold), 4)\n",
    "\n",
    "    # Calculate mean metrics for holdout\n",
    "    mean_accuracy_holdout = round(statistics.mean(Mean_Acc_holdout), 4)\n",
    "    mean_precision_holdout = round(statistics.mean(Mean_Prec_holdout), 4)\n",
    "    mean_recall_holdout = round(statistics.mean(Mean_Rec_holdout), 4)\n",
    "    mean_f1_holdout = round(statistics.mean(Mean_F1_holdout), 4)\n",
    "\n",
    "    # Store results for the current parameter set\n",
    "    results_kfold.append({\n",
    "        'Parameters': param_set,\n",
    "        'Mean Accuracy K-Fold': mean_accuracy_kfold,\n",
    "        'Mean Precision_K-Fold': mean_precision_kfold,\n",
    "        'Mean Recall_K-Fold': mean_recall_kfold,\n",
    "        'Mean F1 K-Fold': mean_f1_kfold,\n",
    "    })\n",
    "\n",
    "    results_holdout.append({\n",
    "        'Parameters': param_set,\n",
    "        'Mean Accuracy Holdout': mean_accuracy_holdout,\n",
    "        'Mean Precision Holdout': mean_precision_holdout,\n",
    "        'Mean Recall Holdout': mean_recall_holdout,\n",
    "        'Mean F1 Holdout': mean_f1_holdout,\n",
    "    })\n",
    "\n",
    "# Create Pandas DataFrames for better visualization\n",
    "results_kfold_df = pd.DataFrame(results_kfold)\n",
    "results_holdout_df = pd.DataFrame(results_holdout)\n",
    "\n",
    "# Tìm chỉ số của dòng có giá trị lớn nhất trong cột \"Mean Accuracy K-Fold\"\n",
    "best_idx = results_kfold_df['Mean Accuracy K-Fold'].idxmax()\n",
    "\n",
    "# Lấy thông số của mô hình tốt nhất\n",
    "best_param_set = results_kfold_df.iloc[best_idx]['Parameters']\n",
    "\n",
    "# Tạo và đào tạo lại mô hình tốt nhất trên toàn bộ tập dữ liệu\n",
    "best_forest_model = RandomForestClassifier(\n",
    "    n_estimators=best_param_set['n_estimators'],\n",
    "    max_depth=best_param_set['max_depth'],\n",
    "    min_samples_split=best_param_set['min_samples_split']\n",
    ")\n",
    "best_forest_model.fit(X, y)\n",
    "\n",
    "# Lưu mô hình vào tệp\n",
    "joblib.dump(best_forest_model, 'best_random_forest_model.joblib')\n",
    "\n",
    "# Draw the k-fold table\n",
    "table_kfold = plt.table(cellText=results_kfold_df.values, colLabels=results_kfold_df.columns, cellLoc='center', loc='center')\n",
    "table_kfold.auto_set_font_size(False)\n",
    "table_kfold.auto_set_column_width(col=list(range(len(results_holdout_df.columns))))\n",
    "table_kfold.set_fontsize(12)\n",
    "table_kfold.scale(2, 2)\n",
    "plt.title('K-Fold Results - Random Forest')\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Draw the holdout table\n",
    "table_holdout = plt.table(cellText=results_holdout_df.values, colLabels=results_holdout_df.columns, cellLoc='center', loc='center')\n",
    "table_holdout.auto_set_font_size(False)\n",
    "table_holdout.auto_set_column_width(col=list(range(len(results_holdout_df.columns))))\n",
    "table_holdout.set_fontsize(12)\n",
    "table_holdout.scale(2, 2)\n",
    "plt.title('Holdout Results - Random Forest')\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "end_time = time.time()\n",
    "print(f'\\nRuntime: {end_time - start_time_overall:.4f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "\n",
    "# Sample data for illustration purposes\n",
    "gender_data = [\"Male\", \"Female\"]\n",
    "occupations = [\n",
    "    \"Nurse\",\n",
    "    \"Doctor\",\n",
    "    \"Engineer\",\n",
    "    \"Lawyer\",\n",
    "    \"Teacher\",\n",
    "    \"Accountant\",\n",
    "    \"Salesperson\",\n",
    "    \"Software Engineer\",\n",
    "    \"Scientist\",\n",
    "    \"Sales Representative\",\n",
    "    \"Manager\"\n",
    "]\n",
    "bmi_category_data = [\"Normal Weight\", \"Normal\", \"Overweight\", \"Obese\"]\n",
    "\n",
    "# Create and fit LabelEncoders for each category\n",
    "gender_encoder = LabelEncoder()\n",
    "gender_encoder.fit(gender_data)\n",
    "\n",
    "occupation_encoder = LabelEncoder()\n",
    "occupation_encoder.fit(occupations)\n",
    "\n",
    "bmi_category_encoder = LabelEncoder()\n",
    "bmi_category_encoder.fit(bmi_category_data)\n",
    "\n",
    "# Save the encoders to files\n",
    "joblib.dump(gender_encoder, \"gender_encoder.joblib\")\n",
    "joblib.dump(occupation_encoder, \"occupation_encoder.joblib\")\n",
    "joblib.dump(bmi_category_encoder, \"bmi_category_encoder.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Features:\n",
      "   Gender  Age  Occupation  Sleep Duration  Quality of Sleep  \\\n",
      "0       1   27           9             6.1                 6   \n",
      "1       1   28           1             6.2                 6   \n",
      "2       1   28           1             6.2                 6   \n",
      "3       1   28           6             5.9                 4   \n",
      "4       1   28           6             5.9                 4   \n",
      "5       1   28           9             5.9                 4   \n",
      "6       1   29          10             6.3                 6   \n",
      "7       1   29           1             7.8                 7   \n",
      "8       1   29           1             7.8                 7   \n",
      "9       1   29           1             7.8                 7   \n",
      "\n",
      "   Physical Activity Level  Stress Level  BMI Category  Heart Rate  \\\n",
      "0                       42             6             3          77   \n",
      "1                       60             8             0          75   \n",
      "2                       60             8             0          75   \n",
      "3                       30             8             2          85   \n",
      "4                       30             8             2          85   \n",
      "5                       30             8             2          85   \n",
      "6                       40             7             2          82   \n",
      "7                       75             6             0          70   \n",
      "8                       75             6             0          70   \n",
      "9                       75             6             0          70   \n",
      "\n",
      "   Daily Steps  Systolic  Diastolic  \n",
      "0         4200       126         83  \n",
      "1        10000       125         80  \n",
      "2        10000       125         80  \n",
      "3         3000       140         90  \n",
      "4         3000       140         90  \n",
      "5         3000       140         90  \n",
      "6         3500       140         90  \n",
      "7         8000       120         80  \n",
      "8         8000       120         80  \n",
      "9         8000       120         80  \n",
      "Processed Class:\n",
      "[2 2 2 1 1 0 0 2 2 2]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 79\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# Initialize and train the Random Forest model\u001b[39;00m\n\u001b[0;32m     74\u001b[0m forest_model \u001b[38;5;241m=\u001b[39m RandomForestClassifier(\n\u001b[0;32m     75\u001b[0m     n_estimators\u001b[38;5;241m=\u001b[39mparam_set[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     76\u001b[0m     max_depth\u001b[38;5;241m=\u001b[39mparam_set[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     77\u001b[0m     min_samples_split\u001b[38;5;241m=\u001b[39mparam_set[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_samples_split\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     78\u001b[0m )\n\u001b[1;32m---> 79\u001b[0m \u001b[43mforest_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m forest_model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# Evaluate the performance of the model for k-fold\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\huuth\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\huuth\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:456\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    445\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    448\u001b[0m ]\n\u001b[0;32m    450\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 456\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    463\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    469\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    471\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    472\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32mc:\\Users\\huuth\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\huuth\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\huuth\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\huuth\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\huuth\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:188\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    186\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[1;32m--> 188\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\huuth\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\huuth\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\tree\\_classes.py:959\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    929\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    930\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    931\u001b[0m \n\u001b[0;32m    932\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    957\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 959\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\huuth\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\tree\\_classes.py:284\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_classification:\n\u001b[1;32m--> 284\u001b[0m     \u001b[43mcheck_classification_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    285\u001b[0m     y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcopy(y)\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\huuth\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:208\u001b[0m, in \u001b[0;36mcheck_classification_targets\u001b[1;34m(y)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_classification_targets\u001b[39m(y):\n\u001b[0;32m    197\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Ensure that target y is of a non-regression type.\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \n\u001b[0;32m    199\u001b[0m \u001b[38;5;124;03m    Only the following target types (as defined in type_of_target) are allowed:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m        Target values.\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     y_type \u001b[38;5;241m=\u001b[39m \u001b[43mtype_of_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[0;32m    210\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    211\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    214\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel-sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    215\u001b[0m     ]:\n\u001b[0;32m    216\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    217\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown label type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Maybe you are trying to fit a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    218\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassifier, which expects discrete classes on a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    219\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregression target with continuous values.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    220\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\huuth\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:388\u001b[0m, in \u001b[0;36mtype_of_target\u001b[1;34m(y, input_name)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;66;03m# Check multiclass\u001b[39;00m\n\u001b[0;32m    387\u001b[0m first_row \u001b[38;5;241m=\u001b[39m y[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m issparse(y) \u001b[38;5;28;01melse\u001b[39;00m y\u001b[38;5;241m.\u001b[39mgetrow(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mdata\n\u001b[1;32m--> 388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mxp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(first_row) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;66;03m# [1, 2, 3] or [[1., 2., 3]] or [[1, 2]]\u001b[39;00m\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m suffix\n\u001b[0;32m    391\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\huuth\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:262\u001b[0m, in \u001b[0;36m_NumPyAPIWrapper.unique_values\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munique_values\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\huuth\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\lib\\arraysetops.py:274\u001b[0m, in \u001b[0;36munique\u001b[1;34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001b[0m\n\u001b[0;32m    272\u001b[0m ar \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(ar)\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 274\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43m_unique1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mequal_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mequal_nan\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unpack_tuple(ret)\n\u001b[0;32m    278\u001b[0m \u001b[38;5;66;03m# axis was specified and not None\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\huuth\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\lib\\arraysetops.py:336\u001b[0m, in \u001b[0;36m_unique1d\u001b[1;34m(ar, return_index, return_inverse, return_counts, equal_nan)\u001b[0m\n\u001b[0;32m    334\u001b[0m     aux \u001b[38;5;241m=\u001b[39m ar[perm]\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 336\u001b[0m     \u001b[43mar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    337\u001b[0m     aux \u001b[38;5;241m=\u001b[39m ar\n\u001b[0;32m    338\u001b[0m mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(aux\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mbool_)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import statistics\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import joblib\n",
    "\n",
    "# Record the start time for overall runtime measurement\n",
    "start_time_overall = time.time()\n",
    "\n",
    "# Load the Sleep_health_and_lifestyle_dataset.csv\n",
    "file_path = 'Sleep_health_and_lifestyle_dataset.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Remove the first column ('person ID')\n",
    "df = df.iloc[:, 1:]\n",
    "\n",
    "# Tách cột \"Blood Pressure\" thành \"Systolic\" và \"Diastolic\"\n",
    "df[['Systolic', 'Diastolic']] = df['Blood Pressure'].str.split('/', expand=True)\n",
    "\n",
    "# Chuyển đổi kiểu dữ liệu sang numeric\n",
    "df['Systolic'] = pd.to_numeric(df['Systolic'], errors='coerce')\n",
    "df['Diastolic'] = pd.to_numeric(df['Diastolic'], errors='coerce')\n",
    "\n",
    "# Xóa cột \"Blood Pressure\" ban đầu\n",
    "df = df.drop('Blood Pressure', axis=1)\n",
    "\n",
    "# Separate features and labels\n",
    "X = df.drop('Sleep Disorder', axis=1)\n",
    "y = df['Sleep Disorder']\n",
    "\n",
    "# Encode 'Occupation', 'BMI Category', and 'Gender' columns\n",
    "columns_to_encode = ['Occupation', 'BMI Category', 'Gender']\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "for column in columns_to_encode:\n",
    "    X[column] = label_encoder.fit_transform(X[column])\n",
    "\n",
    "# Convert labels to numeric format\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# In ra các feature sau khi xử lý dữ liệu\n",
    "print(\"Processed Features:\")\n",
    "print(X.head(10))\n",
    "print(\"Processed Class:\")\n",
    "print(y[:10])\n",
    "\n",
    "\n",
    "# Lists to store metrics\n",
    "results_kfold, results_holdout = [], []\n",
    "\n",
    "for param_set in [\n",
    "    {'n_estimators': 100, 'max_depth': 3, 'min_samples_split': 2},\n",
    "    {'n_estimators': 100, 'max_depth': 3, 'min_samples_split': 3},\n",
    "    {'n_estimators': 100, 'max_depth': 4, 'min_samples_split': 4},\n",
    "    {'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 3},\n",
    "    {'n_estimators': 100, 'max_depth': None, 'min_samples_split': 2},\n",
    "]:\n",
    "    # Initialize arrays to store metrics for the current parameter set\n",
    "    Mean_Acc_kfold, Mean_Prec_kfold, Mean_Rec_kfold, Mean_F1_kfold = [], [], [], []\n",
    "    Mean_Acc_holdout, Mean_Prec_holdout, Mean_Rec_holdout, Mean_F1_holdout = [], [], [], []\n",
    "\n",
    "    # K-fold cross-validation\n",
    "    kf = StratifiedKFold(n_splits=50, shuffle=True, random_state=42)\n",
    "\n",
    "    for train_index, test_index in kf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Initialize and train the Random Forest model\n",
    "        forest_model = RandomForestClassifier(\n",
    "            n_estimators=param_set['n_estimators'],\n",
    "            max_depth=param_set['max_depth'],\n",
    "            min_samples_split=param_set['min_samples_split']\n",
    "        )\n",
    "        forest_model.fit(X_train, y_train)\n",
    "        y_pred = forest_model.predict(X_test)\n",
    "\n",
    "        # Evaluate the performance of the model for k-fold\n",
    "        Accuracy = round(accuracy_score(y_test, y_pred) * 100, 4)\n",
    "        Precision = round(precision_score(y_test, y_pred, average='weighted', zero_division=0) * 100, 4)\n",
    "        Recall = round(recall_score(y_test, y_pred, average='weighted', zero_division=0) * 100, 4)\n",
    "        F1 = round(f1_score(y_test, y_pred, average='weighted', zero_division=0) * 100, 4)\n",
    "\n",
    "        # Append metrics to lists\n",
    "        Mean_Acc_kfold.append(Accuracy)\n",
    "        Mean_Prec_kfold.append(Precision)\n",
    "        Mean_Rec_kfold.append(Recall)\n",
    "        Mean_F1_kfold.append(F1)\n",
    "\n",
    "    # Holdout evaluation\n",
    "    for _ in range(50):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/4, random_state=42)\n",
    "\n",
    "        # Initialize and train the Random Forest model\n",
    "        forest_model = RandomForestClassifier(\n",
    "            n_estimators=param_set['n_estimators'],\n",
    "            max_depth=param_set['max_depth'],\n",
    "            min_samples_split=param_set['min_samples_split']\n",
    "        )\n",
    "        forest_model.fit(X_train, y_train)\n",
    "        y_pred = forest_model.predict(X_test)\n",
    "\n",
    "        # Evaluate the performance of the model for holdout\n",
    "        Accuracy = round(accuracy_score(y_test, y_pred) * 100, 4)\n",
    "        Precision = round(precision_score(y_test, y_pred, average='weighted', zero_division=0) * 100, 4)\n",
    "        Recall = round(recall_score(y_test, y_pred, average='weighted', zero_division=0) * 100, 4)\n",
    "        F1 = round(f1_score(y_test, y_pred, average='weighted', zero_division=0) * 100, 4)\n",
    "\n",
    "        # Append metrics to lists\n",
    "        Mean_Acc_holdout.append(Accuracy)\n",
    "        Mean_Prec_holdout.append(Precision)\n",
    "        Mean_Rec_holdout.append(Recall)\n",
    "        Mean_F1_holdout.append(F1)\n",
    "\n",
    "    # Calculate mean metrics for k-fold\n",
    "    mean_accuracy_kfold = round(statistics.mean(Mean_Acc_kfold), 4)\n",
    "    mean_precision_kfold = round(statistics.mean(Mean_Prec_kfold), 4)\n",
    "    mean_recall_kfold = round(statistics.mean(Mean_Rec_kfold), 4)\n",
    "    mean_f1_kfold = round(statistics.mean(Mean_F1_kfold), 4)\n",
    "\n",
    "    # Calculate mean metrics for holdout\n",
    "    mean_accuracy_holdout = round(statistics.mean(Mean_Acc_holdout), 4)\n",
    "    mean_precision_holdout = round(statistics.mean(Mean_Prec_holdout), 4)\n",
    "    mean_recall_holdout = round(statistics.mean(Mean_Rec_holdout), 4)\n",
    "    mean_f1_holdout = round(statistics.mean(Mean_F1_holdout), 4)\n",
    "\n",
    "    # Store results for the current parameter set\n",
    "    results_kfold.append({\n",
    "        'Parameters': param_set,\n",
    "        'Mean Accuracy K-Fold': mean_accuracy_kfold,\n",
    "        'Mean Precision_K-Fold': mean_precision_kfold,\n",
    "        'Mean Recall_K-Fold': mean_recall_kfold,\n",
    "        'Mean F1 K-Fold': mean_f1_kfold,\n",
    "    })\n",
    "\n",
    "    results_holdout.append({\n",
    "        'Parameters': param_set,\n",
    "        'Mean Accuracy Holdout': mean_accuracy_holdout,\n",
    "        'Mean Precision Holdout': mean_precision_holdout,\n",
    "        'Mean Recall Holdout': mean_recall_holdout,\n",
    "        'Mean F1 Holdout': mean_f1_holdout,\n",
    "    })\n",
    "\n",
    "# Create Pandas DataFrames for better visualization\n",
    "results_kfold_df = pd.DataFrame(results_kfold)\n",
    "results_holdout_df = pd.DataFrame(results_holdout)\n",
    "\n",
    "# Tìm chỉ số của dòng có giá trị lớn nhất trong cột \"Mean Accuracy K-Fold\"\n",
    "best_idx = results_kfold_df['Mean Accuracy K-Fold'].idxmax()\n",
    "\n",
    "# Lấy thông số của mô hình tốt nhất\n",
    "best_param_set = results_kfold_df.iloc[best_idx]['Parameters']\n",
    "\n",
    "# Tạo và đào tạo lại mô hình tốt nhất trên toàn bộ tập dữ liệu\n",
    "best_forest_model = RandomForestClassifier(\n",
    "    n_estimators=best_param_set['n_estimators'],\n",
    "    max_depth=best_param_set['max_depth'],\n",
    "    min_samples_split=best_param_set['min_samples_split']\n",
    ")\n",
    "best_forest_model.fit(X, y)\n",
    "\n",
    "# Lưu mô hình vào tệp\n",
    "joblib.dump(best_forest_model, 'best_random_forest_model.joblib')\n",
    "\n",
    "# Draw the k-fold table\n",
    "table_kfold = plt.table(cellText=results_kfold_df.values, colLabels=results_kfold_df.columns, cellLoc='center', loc='center')\n",
    "table_kfold.auto_set_font_size(False)\n",
    "table_kfold.auto_set_column_width(col=list(range(len(results_holdout_df.columns))))\n",
    "table_kfold.set_fontsize(12)\n",
    "table_kfold.scale(2, 2)\n",
    "plt.title('K-Fold Results - Random Forest')\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Draw the holdout table\n",
    "table_holdout = plt.table(cellText=results_holdout_df.values, colLabels=results_holdout_df.columns, cellLoc='center', loc='center')\n",
    "table_holdout.auto_set_font_size(False)\n",
    "table_holdout.auto_set_column_width(col=list(range(len(results_holdout_df.columns))))\n",
    "table_holdout.set_fontsize(12)\n",
    "table_holdout.scale(2, 2)\n",
    "plt.title('Holdout Results - Random Forest')\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "end_time = time.time()\n",
    "print(f'\\nRuntime: {end_time - start_time_overall:.4f} seconds')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
